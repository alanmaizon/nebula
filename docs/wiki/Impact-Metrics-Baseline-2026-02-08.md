# Impact Metrics Baseline - 2026-02-08

## Goal
Define reproducible enterprise/community impact metrics for the current demo corpus and record baseline values from artifacts.

## Data Source and Reproduction
- Artifact source: `/tmp/nebula-demo-freeze/run-1` and `/tmp/nebula-demo-freeze/run-2`
- Corpus used by the run:
  - `rfp.txt` (requirements corpus)
  - `impact.txt` (program evidence corpus)
- Metric command:
  - `scripts/compute_impact_baseline.py`
- Output artifact:
  - `docs/artifacts/impact-baseline-2026-02-08.json`

## KPI Definitions
| KPI | Definition | Why it matters |
|---|---|---|
| Pipeline success rate | Successful critical endpoint responses / total critical endpoint calls in current demo flow (`upload`, `generate-full-draft`, `export-json`, `export-md`) | Indicates operational reliability for judge/demo execution. |
| Coverage met rate | `coverage.status == met` items / total requirement items | Indicates how completely draft output addresses extracted requirements. |
| Coverage missing rate | `coverage.status == missing` items / total requirement items | Indicates unresolved requirement risk at draft time. |
| Citation density | Total citations / total draft paragraphs | Indicates grounding density and auditability of generated text. |
| Unsupported claim rate | `missing_evidence` items / (`paragraphs` + `missing_evidence` items) | Indicates unsupported-output risk presented to users. |
| Requirements extracted per run | Total extracted requirements / run count | Indicates baseline extraction depth on current corpus. |

## Baseline Results (Current Corpus)
| KPI | Baseline value | Source field |
|---|---|---|
| Pipeline success rate | `100.0%` | `metrics.pipeline_success_rate_pct` |
| Coverage met rate | `100.0%` | `metrics.coverage_met_rate_pct` |
| Coverage missing rate | `0.0%` | `metrics.coverage_missing_rate_pct` |
| Citation density | `1.0 citations/paragraph` | `metrics.citation_density_per_paragraph` |
| Unsupported claim rate | `0.0%` | `metrics.unsupported_claim_rate_pct` |
| Requirements extracted per run | `2.0` | `metrics.requirements_per_run_avg` |

## Method Notes
- Baseline is computed from artifact files generated by `scripts/run_demo_freeze.sh`.
- Metric extraction uses canonical fields in `export.json -> bundle.json` (`requirements.questions`, `coverage.items`, `drafts.*.draft.paragraphs`, `missing_evidence`).
- `full_draft.json` is validated as a contract guard (`requirements`, `section_runs`, `coverage.items`, `export.bundle.json`, `run_summary.status=complete`).
- Metrics are deterministic given the same artifact inputs and calculation script.
- This baseline is corpus-specific and should be refreshed when corpus, prompts, or runtime behavior changes.

## Submission and Demo Impact Bullets
- Nebula achieved `100%` successful execution across all critical demo endpoints over two full artifacted runs.
- Baseline coverage shows `100%` requirements in `met` status for the current demo corpus, with `0%` missing coverage items.
- Draft grounding quality baseline is `1.0` citation per paragraph with `0%` unsupported claims flagged.
- Each run consistently extracted `2` structured requirements from the same RFP corpus, showing repeatable output shape.
- The metrics are reproducible from stored artifacts (`docs/artifacts/impact-baseline-2026-02-08.json`), not anecdotal observations.
