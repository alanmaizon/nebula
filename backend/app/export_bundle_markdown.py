from __future__ import annotations

import json

from .export_bundle_common import (
    _as_dict_list,
    _as_optional_dict,
    _coerce_positive_int,
    _escape_pipe,
    _normalize_optional_id,
)
from .export_bundle_metrics import _uncertainty_penalty_factor


def _build_markdown_files(
    *,
    profile: str,
    include_debug: bool,
    project: dict[str, object],
    export_request: dict[str, object],
    documents: list[dict[str, object]],
    requirements: dict[str, object] | None,
    drafts: dict[str, dict[str, object]],
    selected_sections: list[str],
    coverage_items: list[dict[str, object]],
    coverage_counts: dict[str, int],
    uncertainty: dict[str, object],
    missing_evidence: list[dict[str, object]],
    validations: dict[str, object],
    run_metadata: dict[str, object],
    summary: dict[str, object],
) -> list[dict[str, str]]:
    files: list[dict[str, str]] = []

    requirement_rows = _build_requirement_rows(requirements, coverage_items)
    draft_application = _render_draft_application_markdown(selected_sections, drafts)
    requirements_matrix = _render_requirements_matrix_markdown(requirement_rows)
    coverage_markdown = _render_coverage_markdown(coverage_items, coverage_counts, uncertainty)
    missing_evidence_markdown = _render_missing_evidence_markdown(missing_evidence)
    validation_markdown = _render_validation_markdown(validations, include_debug)

    if profile == "hackathon":
        files.append(
            {
                "path": "README_EXPORT.md",
                "content": _render_hackathon_readme(
                    project=project,
                    export_request=export_request,
                    summary=summary,
                    documents=documents,
                ),
            }
        )
        files.append({"path": "REQUIREMENTS_MATRIX.md", "content": requirements_matrix})
        files.append({"path": "DRAFT_APPLICATION.md", "content": draft_application})
        files.append({"path": "COVERAGE.md", "content": coverage_markdown})
        files.append({"path": "MISSING_EVIDENCE.md", "content": missing_evidence_markdown})
    elif profile == "submission":
        files.append({"path": "application.md", "content": draft_application})
        files.append(
            {
                "path": "requirements.md",
                "content": requirements_matrix,
            }
        )
        files.append({"path": "coverage.md", "content": coverage_markdown})
        files.append({"path": "missing_evidence.md", "content": missing_evidence_markdown})
    else:  # internal
        files.append({"path": "application.md", "content": draft_application})
        files.append({"path": "requirements.md", "content": requirements_matrix})
        files.append({"path": "coverage.md", "content": coverage_markdown})
        files.append({"path": "missing_evidence.md", "content": missing_evidence_markdown})
        if validation_markdown:
            files.append({"path": "validation.md", "content": validation_markdown})
        if include_debug:
            files.append(
                {
                    "path": "DEBUG_RUN.json",
                    "content": json.dumps(
                        {
                            "run_metadata": run_metadata,
                            "validations": validations,
                        },
                        ensure_ascii=False,
                        indent=2,
                    ),
                }
            )

    return [file for file in files if str(file.get("content") or "").strip()]


def _render_hackathon_readme(
    *,
    project: dict[str, object],
    export_request: dict[str, object],
    summary: dict[str, object],
    documents: list[dict[str, object]],
) -> str:
    lines = [
        "# Nebula Export Bundle",
        "",
        "This bundle is generated by the final-stage export packager with cite-first constraints and traceability.",
        "",
        "## Project",
        f"- ID: `{project.get('id', '')}`",
        f"- Name: {project.get('name', '')}",
        f"- Profile: {export_request.get('profile', 'hackathon')}",
        "",
        "## Summary",
        f"- Overall completion: {summary.get('overall_completion', 'unknown')}",
        f"- Coverage (met/partial/missing): {summary.get('coverage_overview', {}).get('met', 0)}/"
        f"{summary.get('coverage_overview', {}).get('partial', 0)}/"
        f"{summary.get('coverage_overview', {}).get('missing', 0)}",
        f"- Unsupported claims: {summary.get('unsupported_claims_count', 0)}",
        f"- Missing evidence items: {summary.get('missing_evidence_count', 0)}",
        "",
        "## Inputs",
        f"- Documents: {len(documents)}",
        "",
        "## Files",
        "- `REQUIREMENTS_MATRIX.md`",
        "- `DRAFT_APPLICATION.md`",
        "- `COVERAGE.md`",
        "- `MISSING_EVIDENCE.md`",
    ]
    return "\n".join(lines).strip()


def _render_requirements_matrix_markdown(rows: list[dict[str, str]]) -> str:
    lines = [
        "# Requirements Matrix",
        "",
        "| internal_id | original_id | requirement | status | notes |",
        "|---|---|---|---|---|",
    ]
    if not rows:
        lines.append("| n/a |  | No requirements available | missing |  |")
    else:
        for row in rows:
            lines.append(
                "| "
                + " | ".join(
                    [
                        _escape_pipe(row.get("internal_id", "")),
                        _escape_pipe(row.get("original_id", "")),
                        _escape_pipe(row.get("requirement", "")),
                        _escape_pipe(row.get("status", "")),
                        _escape_pipe(row.get("notes", "")),
                    ]
                )
                + " |"
            )
    return "\n".join(lines).strip()


def _render_draft_application_markdown(
    selected_sections: list[str],
    drafts: dict[str, dict[str, object]],
) -> str:
    lines = ["# Draft Application", ""]
    if not selected_sections:
        lines.append("_No draft sections available._")
        return "\n".join(lines).strip()

    for section_key in selected_sections:
        section_entry = drafts.get(section_key) or {}
        draft = _as_optional_dict(section_entry.get("draft")) or {}
        paragraphs = _as_dict_list(draft.get("paragraphs"))
        missing = _as_dict_list(draft.get("missing_evidence"))

        lines.append(f"## {section_key}")
        lines.append("")
        if not paragraphs:
            lines.append("_No draft paragraphs available._")
            lines.append("")
        else:
            for index, paragraph in enumerate(paragraphs, start=1):
                text = str(paragraph.get("text") or "").strip()
                unsupported = bool(paragraph.get("unsupported", False))
                if unsupported and "[UNSUPPORTED]" not in text:
                    text = f"{text} [UNSUPPORTED]"
                lines.append(f"{index}. {text}")
            lines.append("")

        citations = _collect_section_citations(paragraphs)
        lines.append("### Citations")
        if citations:
            for citation in citations:
                snippet = citation.get("snippet", "")
                lines.append(
                    f"- `{citation.get('doc_id', '')}` p{citation.get('page', '?')}: {snippet}"
                )
        else:
            lines.append("- None")
        lines.append("")

        unsupported_details = _unsupported_paragraph_descriptions(paragraphs)
        if unsupported_details or missing:
            lines.append("### Unsupported / Missing")
            for detail in unsupported_details:
                lines.append(f"- {detail}")
            for item in missing:
                claim = str(item.get("claim") or "Missing evidence item").strip()
                suggestion = str(item.get("suggested_upload") or "Upload more evidence.").strip()
                lines.append(f"- {claim} (suggested upload: {suggestion})")
            lines.append("")

    return "\n".join(lines).strip()


def _render_coverage_markdown(
    coverage_items: list[dict[str, object]],
    coverage_counts: dict[str, int],
    uncertainty: dict[str, object],
) -> str:
    met = coverage_counts.get("met", 0)
    partial = coverage_counts.get("partial", 0)
    missing = coverage_counts.get("missing", 0)
    total = met + partial + missing

    unsupported_claims = _coerce_positive_int(uncertainty.get("unsupported_claims_count"))
    citation_mismatch_count = _coerce_positive_int(uncertainty.get("citation_mismatch_count"))
    empty_required_sections_count = _coerce_positive_int(uncertainty.get("empty_required_sections_count"))
    uncertainty_penalty = _uncertainty_penalty_factor(
        total=max(total, 1),
        unsupported_claims_count=unsupported_claims or 0,
        citation_mismatch_count=citation_mismatch_count or 0,
        empty_required_sections_count=empty_required_sections_count or 0,
    )
    readiness_base = ((met + 0.5 * partial) / total * 100) if total > 0 else 0.0
    completion_base = (met / total * 100) if total > 0 else 0.0
    readiness = readiness_base * (1.0 - uncertainty_penalty)
    completion = completion_base * (1.0 - uncertainty_penalty)

    lines = [
        "# Coverage Summary",
        "",
        f"- Readiness score: {readiness:.1f}%",
        f"- Completion score: {completion:.1f}%",
        f"- Met: {met}",
        f"- Partial: {partial}",
        f"- Missing: {missing}",
    ]
    source_conflict_count = _coerce_positive_int(uncertainty.get("source_conflict_count")) or 0
    source_ambiguity_count = _coerce_positive_int(uncertainty.get("source_ambiguity_count")) or 0
    if source_ambiguity_count > 0:
        lines.append(f"- Source ambiguity warnings: {source_ambiguity_count}")
    if source_conflict_count > 0:
        lines.append(f"- Source conflicts detected: {source_conflict_count}")
    if citation_mismatch_count:
        lines.append(f"- Citation mismatches detected: {citation_mismatch_count}")
    if empty_required_sections_count:
        lines.append(f"- Empty required sections detected: {empty_required_sections_count}")
    if unsupported_claims:
        lines.append(f"- Unsupported claims detected: {unsupported_claims}")

    recommendations: list[str] = []
    if source_ambiguity_count > 0:
        recommendations.append(
            "Resolve source ambiguity warning: select a single primary RFP document for deterministic extraction."
        )
    if source_conflict_count > 0:
        recommendations.append(
            "Resolve source conflict warning: align contradictory evidence before finalizing scores."
        )
    if (citation_mismatch_count or 0) > 0:
        recommendations.append(
            "Resolve citation mismatch warning: fix doc/page/snippet integrity and inline-vs-structured citations."
        )
    if (empty_required_sections_count or 0) > 0:
        recommendations.append(
            "Address empty required section warning: add grounded content for missing required sections."
        )
    for item in coverage_items:
        status = str(item.get("status") or "").strip().lower()
        if status not in {"missing", "partial"}:
            continue
        req_id = str(item.get("requirement_id") or "").strip() or "unknown"
        notes = str(item.get("notes") or "").strip()
        if not notes:
            notes = "Needs additional supported draft coverage."
        recommendations.append(f"`{req_id}`: {notes}")

    if recommendations:
        lines.extend(["", "## Recommended Next Edits"])
        for recommendation in recommendations[:10]:
            lines.append(f"- {recommendation}")
    else:
        lines.extend(["", "## Recommended Next Edits", "- No high-priority gaps detected."])

    return "\n".join(lines).strip()


def _render_missing_evidence_markdown(missing_evidence: list[dict[str, object]]) -> str:
    if not missing_evidence:
        return ""

    lines = [
        "# Missing Evidence",
        "",
        "| item | why | suggested uploads | impacts |",
        "|---|---|---|---|",
    ]
    for item in missing_evidence:
        claim = str(item.get("claim") or item.get("item") or "Unknown").strip()
        reason = str(item.get("reason") or item.get("why") or "Evidence gap").strip()
        suggested = item.get("suggested_doc_types")
        if isinstance(suggested, list):
            suggested_text = ", ".join(str(part) for part in suggested)
        else:
            suggested_text = str(item.get("suggested_upload") or "").strip() or "Upload supporting evidence"
        impacts = item.get("affected_requirements") or item.get("affected_sections") or item.get("impacts") or []
        if isinstance(impacts, list):
            impacts_text = ", ".join(str(part) for part in impacts) or "n/a"
        else:
            impacts_text = str(impacts).strip() or "n/a"

        lines.append(
            "| "
            + " | ".join(
                [
                    _escape_pipe(claim),
                    _escape_pipe(reason),
                    _escape_pipe(suggested_text),
                    _escape_pipe(impacts_text),
                ]
            )
            + " |"
        )
    return "\n".join(lines).strip()


def _render_validation_markdown(validations: dict[str, object], include_debug: bool) -> str:
    if not validations:
        return ""
    if include_debug:
        return "# Validation\n\n```json\n" + json.dumps(validations, ensure_ascii=False, indent=2) + "\n```"

    failures: list[str] = []
    for key, value in validations.items():
        if isinstance(value, dict):
            repaired = value.get("repaired")
            errors = value.get("errors")
            if repaired or (isinstance(errors, list) and len(errors) > 0):
                failures.append(f"- {key}: repaired={bool(repaired)}, errors={len(errors) if isinstance(errors, list) else 0}")
    if not failures:
        return ""
    return "# Validation\n\n" + "\n".join(failures)


def _build_requirement_rows(
    requirements: dict[str, object] | None,
    coverage_items: list[dict[str, object]],
) -> list[dict[str, str]]:
    rows: list[dict[str, str]] = []
    coverage_lookup: dict[str, dict[str, object]] = {}
    for item in coverage_items:
        if not isinstance(item, dict):
            continue
        aliases = {
            str(item.get("requirement_id") or "").strip(),
            str(item.get("internal_id") or "").strip(),
            str(item.get("original_id") or "").strip(),
        }
        for alias in aliases:
            if alias:
                coverage_lookup[alias] = item
    seen_internal_ids: set[str] = set()

    if requirements:
        questions = requirements.get("questions")
        if isinstance(questions, list):
            for index, question in enumerate(questions, start=1):
                if not isinstance(question, dict):
                    continue
                req_id = (
                    str(question.get("internal_id") or "").strip()
                    or str(question.get("id") or "").strip()
                    or f"Q{index}"
                )
                original_id = _normalize_optional_id(question.get("original_id"))
                prompt = str(question.get("prompt") or "").strip() or f"Question {index}"
                rows.append(_build_row(req_id, original_id, prompt, coverage_lookup.get(req_id)))
                seen_internal_ids.add(req_id)

        attachments = requirements.get("required_attachments")
        if isinstance(attachments, list):
            attachment_index = 1
            for attachment in attachments:
                text = str(attachment).strip()
                if not text:
                    continue
                req_id = f"A{attachment_index}"
                rows.append(_build_row(req_id, None, text, coverage_lookup.get(req_id)))
                seen_internal_ids.add(req_id)
                attachment_index += 1

        for key_prefix, source_key in [("E", "eligibility"), ("R", "rubric"), ("D", "disallowed_costs")]:
            entries = requirements.get(source_key)
            if not isinstance(entries, list):
                continue
            for index, value in enumerate(entries, start=1):
                text = str(value).strip()
                if not text:
                    continue
                req_id = f"{key_prefix}{index}"
                rows.append(_build_row(req_id, None, text, coverage_lookup.get(req_id)))
                seen_internal_ids.add(req_id)

    for item in coverage_items:
        req_id = (
            str(item.get("internal_id") or "").strip()
            or str(item.get("requirement_id") or "").strip()
        )
        if not req_id or req_id in seen_internal_ids:
            continue
        rows.append(
            _build_row(
                req_id,
                _normalize_optional_id(item.get("original_id")),
                f"Unknown requirement ({req_id})",
                item,
            )
        )
        seen_internal_ids.add(req_id)

    return rows


def _build_row(
    internal_id: str,
    original_id: str | None,
    requirement: str,
    coverage_item: dict[str, object] | None,
) -> dict[str, str]:
    if not coverage_item:
        return {
            "internal_id": internal_id,
            "original_id": original_id or "",
            "requirement": requirement,
            "status": "missing",
            "notes": "No coverage item returned.",
        }
    return {
        "internal_id": str(coverage_item.get("internal_id") or coverage_item.get("requirement_id") or internal_id),
        "original_id": _normalize_optional_id(coverage_item.get("original_id")) or original_id or "",
        "requirement": requirement,
        "status": str(coverage_item.get("status") or "missing"),
        "notes": str(coverage_item.get("notes") or ""),
    }


def _collect_section_citations(paragraphs: list[dict[str, object]]) -> list[dict[str, object]]:
    citations: list[dict[str, object]] = []
    seen: set[str] = set()
    for paragraph in paragraphs:
        refs = paragraph.get("citations")
        if not isinstance(refs, list):
            continue
        for citation in refs:
            if not isinstance(citation, dict):
                continue
            key = (
                str(citation.get("doc_id") or "").strip(),
                str(citation.get("page") or "").strip(),
                str(citation.get("snippet") or "").strip(),
            )
            encoded = "|".join(key)
            if encoded in seen:
                continue
            seen.add(encoded)
            citations.append(
                {
                    "doc_id": key[0],
                    "page": key[1],
                    "snippet": key[2],
                }
            )
    return citations


def _unsupported_paragraph_descriptions(paragraphs: list[dict[str, object]]) -> list[str]:
    messages: list[str] = []
    for index, paragraph in enumerate(paragraphs, start=1):
        if bool(paragraph.get("unsupported", False)):
            messages.append(f"Paragraph {index} has no grounded citations.")
    return messages
